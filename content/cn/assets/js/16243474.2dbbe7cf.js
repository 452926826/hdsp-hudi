"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[98020],{15680:(e,t,a)=>{a.d(t,{xA:()=>g,yg:()=>c});var n=a(96540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},g=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),p=d(a),u=r,c=p["".concat(s,".").concat(u)]||p[u]||m[u]||i;return a?n.createElement(c,o(o({ref:t},g),{},{components:a})):n.createElement(c,o({ref:t},g))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:r,o[1]=l;for(var d=2;d<i;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},26982:(e,t,a)=>{a.d(t,{A:()=>r});var n=a(96540);const r=function(e){let{children:t,hidden:a,className:r}=e;return n.createElement("div",{role:"tabpanel",hidden:a,className:r},t)}},3593:(e,t,a)=>{a.d(t,{A:()=>m});var n=a(58168),r=a(96540),i=a(92303),o=a(48555);const l=function(){const e=(0,r.useContext)(o.A);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e};var s=a(91211),d=a(20053);const g={tabItem:"tabItem_vU9c"};function p(e){const{lazy:t,block:a,defaultValue:i,values:o,groupId:p,className:m}=e,u=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),c=o??u.map((e=>{let{props:{value:t,label:a,attributes:n}}=e;return{value:t,label:a,attributes:n}})),h=(0,s.XI)(c,((e,t)=>e.value===t.value));if(h.length>0)throw new Error(`Docusaurus error: Duplicate values "${h.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const y=null===i?i:i??u.find((e=>e.props.default))?.props.value??u[0]?.props.value;if(null!==y&&!c.some((e=>e.value===y)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${y}" but none of its children has the corresponding value. Available values are: ${c.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:f,setTabGroupChoices:N}=l(),[b,v]=(0,r.useState)(y),w=[],{blockElementScrollPositionUntilNextRender:k}=(0,s.a_)();if(null!=p){const e=f[p];null!=e&&e!==b&&c.some((t=>t.value===e))&&v(e)}const C=e=>{const t=e.currentTarget,a=w.indexOf(t),n=c[a].value;n!==b&&(k(t),v(n),null!=p&&N(p,n))},S=e=>{let t=null;switch(e.key){case"ArrowRight":{const a=w.indexOf(e.currentTarget)+1;t=w[a]||w[0];break}case"ArrowLeft":{const a=w.indexOf(e.currentTarget)-1;t=w[a]||w[w.length-1];break}}t?.focus()};return r.createElement("div",{className:"tabs-container"},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,d.A)("tabs",{"tabs--block":a},m)},c.map((e=>{let{value:t,label:a,attributes:i}=e;return r.createElement("li",(0,n.A)({role:"tab",tabIndex:b===t?0:-1,"aria-selected":b===t,key:t,ref:e=>w.push(e),onKeyDown:S,onFocus:C,onClick:C},i,{className:(0,d.A)("tabs__item",g.tabItem,i?.className,{"tabs__item--active":b===t})}),a??t)}))),t?(0,r.cloneElement)(u.filter((e=>e.props.value===b))[0],{className:"margin-vert--md"}):r.createElement("div",{className:"margin-vert--md"},u.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==b})))))}function m(e){const t=(0,i.A)();return r.createElement(p,(0,n.A)({key:String(t)},e))}},41202:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>s,default:()=>u,frontMatter:()=>l,metadata:()=>d,toc:()=>g});var n=a(58168),r=(a(96540),a(15680)),i=a(3593),o=a(26982);const l={title:"Streaming Ingestion",keywords:["hudi","streamer","hoodiestreamer","spark_streaming"]},s=void 0,d={unversionedId:"hoodie_streaming_ingestion",id:"hoodie_streaming_ingestion",title:"Streaming Ingestion",description:"Hudi Streamer",source:"@site/docs/hoodie_streaming_ingestion.md",sourceDirName:".",slug:"/hoodie_streaming_ingestion",permalink:"/cn/docs/next/hoodie_streaming_ingestion",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/docs/hoodie_streaming_ingestion.md",tags:[],version:"current",frontMatter:{title:"Streaming Ingestion",keywords:["hudi","streamer","hoodiestreamer","spark_streaming"]},sidebar:"docs",previous:{title:"\u5199\u5165 Hudi \u6570\u636e\u96c6",permalink:"/cn/docs/next/writing_data"},next:{title:"SQL Procedures",permalink:"/cn/docs/next/procedures"}},g=[{value:"Hudi Streamer",id:"hudi-streamer",children:[{value:"Options",id:"options",children:[],level:3},{value:"Using <code>hudi-utilities</code> bundle jars",id:"using-hudi-utilities-bundle-jars",children:[],level:3},{value:"Concurrency Control",id:"concurrency-control",children:[],level:3},{value:"Checkpointing",id:"checkpointing",children:[],level:3},{value:"Transformers",id:"transformers",children:[],level:3},{value:"Schema Providers",id:"schema-providers",children:[{value:"Schema Registry Provider",id:"schema-registry-provider",children:[],level:4},{value:"JDBC Schema Provider",id:"jdbc-schema-provider",children:[],level:4},{value:"File Based Schema Provider",id:"file-based-schema-provider",children:[],level:4},{value:"Hive Schema Provider",id:"hive-schema-provider",children:[],level:4},{value:"Schema Provider with Post Processor",id:"schema-provider-with-post-processor",children:[],level:4}],level:3},{value:"Sources",id:"sources",children:[{value:"Distributed File System (DFS)",id:"distributed-file-system-dfs",children:[],level:4},{value:"Kafka",id:"kafka",children:[],level:4},{value:"Pulsar",id:"pulsar",children:[],level:4},{value:"Cloud storage event sources",id:"cloud-storage-event-sources",children:[{value:"AWS Setup",id:"aws-setup",children:[],level:5}],level:4},{value:"JDBC Source",id:"jdbc-source",children:[],level:4},{value:"SQL Sources",id:"sql-sources",children:[],level:4}],level:3},{value:"Error Table",id:"error-table",children:[],level:3},{value:"Termination Strategy",id:"termination-strategy",children:[],level:3},{value:"Dynamic configuration updates",id:"dynamic-configuration-updates",children:[],level:3}],level:2},{value:"MultiTableStreamer",id:"multitablestreamer",children:[],level:2},{value:"Structured Streaming",id:"structured-streaming",children:[{value:"Streaming Write",id:"streaming-write",children:[],level:3},{value:"Streaming Read",id:"streaming-read",children:[],level:3}],level:2},{value:"Flink Ingestion",id:"flink-ingestion",children:[{value:"CDC Ingestion",id:"cdc-ingestion",children:[],level:3},{value:"Bulk Insert",id:"bulk-insert",children:[{value:"Options",id:"options-1",children:[],level:4}],level:3},{value:"Index Bootstrap",id:"index-bootstrap",children:[{value:"Options",id:"options-2",children:[],level:4},{value:"How To Use",id:"how-to-use",children:[],level:4}],level:3},{value:"Changelog Mode",id:"changelog-mode",children:[{value:"Options",id:"options-3",children:[],level:4}],level:3},{value:"Append Mode",id:"append-mode",children:[{value:"Inline Clustering",id:"inline-clustering",children:[],level:4},{value:"Async Clustering",id:"async-clustering",children:[],level:4},{value:"Clustering Plan Strategy",id:"clustering-plan-strategy",children:[],level:4}],level:3},{value:"Bucket Index",id:"bucket-index",children:[{value:"Options",id:"options-4",children:[],level:4}],level:3},{value:"Rate Limit",id:"rate-limit",children:[{value:"Options",id:"options-5",children:[],level:4}],level:3}],level:2},{value:"Kafka Connect Sink",id:"kafka-connect-sink",children:[],level:2}],p={toc:g},m="wrapper";function u(e){let{components:t,...l}=e;return(0,r.yg)(m,(0,n.A)({},p,l,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h2",{id:"hudi-streamer"},"Hudi Streamer"),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer")," utility (part of ",(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities-slim-bundle")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities-bundle"),") provides ways to ingest\nfrom different sources such as DFS or Kafka, with the following capabilities."),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Exactly once ingestion of new events from\nKafka, ",(0,r.yg)("a",{parentName:"li",href:"https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide#_incremental_imports"},"incremental imports")," from Sqoop or\noutput of ",(0,r.yg)("inlineCode",{parentName:"li"},"HiveIncrementalPuller")," or files under a DFS folder"),(0,r.yg)("li",{parentName:"ul"},"Support json, avro or a custom record types for the incoming data"),(0,r.yg)("li",{parentName:"ul"},"Manage checkpoints, rollback & recovery"),(0,r.yg)("li",{parentName:"ul"},"Leverage Avro schemas from DFS or Confluent ",(0,r.yg)("a",{parentName:"li",href:"https://github.com/confluentinc/schema-registry"},"schema registry"),"."),(0,r.yg)("li",{parentName:"ul"},"Support for plugging in transformations")),(0,r.yg)("div",{className:"admonition admonition-danger alert alert--danger"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Important")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"The following classes were renamed and relocated to ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.streamer")," package."),(0,r.yg)("ul",{parentName:"div"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"DeltastreamerMultiWriterCkptUpdateFunc")," is renamed to ",(0,r.yg)("inlineCode",{parentName:"li"},"StreamerMultiWriterCkptUpdateFunc")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"DeltaSync")," is renamed to ",(0,r.yg)("inlineCode",{parentName:"li"},"StreamSync")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"HoodieDeltaStreamer")," is renamed to ",(0,r.yg)("inlineCode",{parentName:"li"},"HoodieStreamer")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"HoodieDeltaStreamerMetrics")," is renamed to ",(0,r.yg)("inlineCode",{parentName:"li"},"HoodieStreamerMetrics")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"HoodieMultiTableDeltaStreamer")," is renamed to ",(0,r.yg)("inlineCode",{parentName:"li"},"HoodieMultiTableStreamer"))),(0,r.yg)("p",{parentName:"div"},"To maintain backward compatibility, the original classes are still present in the ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.deltastreamer"),"\npackage, but have been deprecated."))),(0,r.yg)("h3",{id:"options"},"Options"),(0,r.yg)("details",null,(0,r.yg)("summary",null,'Expand this to see HoodieStreamer\'s "--help" output describing its capabilities in more details.'),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},'[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` --help\nUsage: <main class> [options]\n  Options:\n    --allow-commit-on-no-checkpoint-change\n      allow commits even if checkpoint has not changed before and after fetch\n      datafrom source. This might be useful in sources like SqlSource where\n      there is not checkpoint. And is not recommended to enable in continuous\n      mode.\n      Default: false\n    --base-file-format\n      File format for the base files. PARQUET (or) HFILE\n      Default: PARQUET\n    --bootstrap-index-class\n      subclass of BootstrapIndex\n      Default: org.apache.hudi.common.bootstrap.index.HFileBootstrapIndex\n    --bootstrap-overwrite\n      Overwrite existing target table, default false\n      Default: false\n    --checkpoint\n      Resume Hudi Streamer from this checkpoint.\n    --cluster-scheduling-minshare\n      Minshare for clustering as defined in\n      https://spark.apache.org/docs/latest/job-scheduling.html\n      Default: 0\n    --cluster-scheduling-weight\n      Scheduling weight for clustering as defined in\n      https://spark.apache.org/docs/latest/job-scheduling.html\n      Default: 1\n    --commit-on-errors\n      Commit even when some records failed to be written\n      Default: false\n    --compact-scheduling-minshare\n      Minshare for compaction as defined in\n      https://spark.apache.org/docs/latest/job-scheduling.html\n      Default: 0\n    --compact-scheduling-weight\n      Scheduling weight for compaction as defined in\n      https://spark.apache.org/docs/latest/job-scheduling.html\n      Default: 1\n    --config-hot-update-strategy-class\n      Configuration hot update in continuous mode\n      Default: <empty string>\n    --continuous\n      Hudi Streamer runs in continuous mode running source-fetch -> Transform\n      -> Hudi Write in loop\n      Default: false\n    --delta-sync-scheduling-minshare\n      Minshare for delta sync as defined in\n      https://spark.apache.org/docs/latest/job-scheduling.html\n      Default: 0\n    --delta-sync-scheduling-weight\n      Scheduling weight for delta sync as defined in\n      https://spark.apache.org/docs/latest/job-scheduling.html\n      Default: 1\n    --disable-compaction\n      Compaction is enabled for MoR table by default. This flag disables it\n      Default: false\n    --enable-hive-sync\n      Enable syncing to hive\n      Default: false\n    --enable-sync\n      Enable syncing meta\n      Default: false\n    --filter-dupes\n      Should duplicate records from source be dropped/filtered out before\n      insert/bulk-insert\n      Default: false\n    --force-empty-sync\n      Force syncing meta even on empty commit\n      Default: false\n    --help, -h\n\n    --hoodie-conf\n      Any configuration that can be set in the properties file (using the CLI\n      parameter "--props") can also be passed command line using this\n      parameter. This can be repeated\n      Default: []\n    --ingestion-metrics-class\n      Ingestion metrics class for reporting metrics during ingestion\n      lifecycles.\n      Default: org.apache.hudi.utilities.streamer.HoodieStreamerMetrics\n    --initial-checkpoint-provider\n      subclass of\n      org.apache.hudi.utilities.checkpointing.InitialCheckpointProvider.\n      Generate check point for Hudi Streamer for the first run. This field\n      will override the checkpoint of last commit using the checkpoint field.\n      Use this field only when switching source, for example, from DFS source\n      to Kafka Source.\n    --max-pending-clustering\n      Maximum number of outstanding inflight/requested clustering. Delta Sync\n      will not happen unlessoutstanding clustering is less than this number\n      Default: 5\n    --max-pending-compactions\n      Maximum number of outstanding inflight/requested compactions. Delta Sync\n      will not happen unlessoutstanding compactions is less than this number\n      Default: 5\n    --max-retry-count\n      the max retry count if --retry-on-source-failures is enabled\n      Default: 3\n    --min-sync-interval-seconds\n      the min sync interval of each sync in continuous mode\n      Default: 0\n    --op\n      Takes one of these values : UPSERT (default), INSERT, BULK_INSERT,\n      INSERT_OVERWRITE, INSERT_OVERWRITE_TABLE, DELETE_PARTITION\n      Default: UPSERT\n      Possible Values: [INSERT, INSERT_PREPPED, UPSERT, UPSERT_PREPPED, BULK_INSERT, BULK_INSERT_PREPPED, DELETE, DELETE_PREPPED, BOOTSTRAP, INSERT_OVERWRITE, CLUSTER, DELETE_PARTITION, INSERT_OVERWRITE_TABLE, COMPACT, INDEX, ALTER_SCHEMA, LOG_COMPACT, UNKNOWN]\n    --payload-class\n      subclass of HoodieRecordPayload, that works off a GenericRecord.\n      Implement your own, if you want to do something other than overwriting\n      existing value\n      Default: org.apache.hudi.common.model.OverwriteWithLatestAvroPayload\n    --post-write-termination-strategy-class\n      Post writer termination strategy class to gracefully shutdown\n      deltastreamer in continuous mode\n      Default: <empty string>\n    --props\n      path to properties file on localfs or dfs, with configurations for\n      hoodie client, schema provider, key generator and data source. For\n      hoodie client props, sane defaults are used, but recommend use to\n      provide basic things like metrics endpoints, hive configs etc. For\n      sources, referto individual classes, for supported properties.\n      Properties in this file can be overridden by "--hoodie-conf"\n      Default: file:///Users/shiyanxu/src/test/resources/streamer-config/dfs-source.properties\n    --retry-interval-seconds\n      the retry interval for source failures if --retry-on-source-failures is\n      enabled\n      Default: 30\n    --retry-last-pending-inline-clustering, -rc\n      Retry last pending inline clustering plan before writing to sink.\n      Default: false\n    --retry-last-pending-inline-compaction\n      Retry last pending inline compaction plan before writing to sink.\n      Default: false\n    --retry-on-source-failures\n      Retry on any source failures\n      Default: false\n    --run-bootstrap\n      Run bootstrap if bootstrap index is not found\n      Default: false\n    --schemaprovider-class\n      subclass of org.apache.hudi.utilities.schema.SchemaProvider to attach\n      schemas to input & target table data, built in options:\n      org.apache.hudi.utilities.schema.FilebasedSchemaProvider.Source (See\n      org.apache.hudi.utilities.sources.Source) implementation can implement\n      their own SchemaProvider. For Sources that return Dataset<Row>, the\n      schema is obtained implicitly. However, this CLI option allows\n      overriding the schemaprovider returned by Source.\n    --source-class\n      Subclass of org.apache.hudi.utilities.sources to read data. Built-in\n      options: org.apache.hudi.utilities.sources.{JsonDFSSource (default),\n      AvroDFSSource, JsonKafkaSource, AvroKafkaSource, HiveIncrPullSource}\n      Default: org.apache.hudi.utilities.sources.JsonDFSSource\n    --source-limit\n      Maximum amount of data to read from source. Default: No limit, e.g:\n      DFS-Source => max bytes to read, Kafka-Source => max events to read\n      Default: 9223372036854775807\n    --source-ordering-field\n      Field within source record to decide how to break ties between records\n      with same key in input data. Default: \'ts\' holding unix timestamp of\n      record\n      Default: ts\n    --spark-master\n      spark master to use, if not defined inherits from your environment\n      taking into account Spark Configuration priority rules (e.g. not using\n      spark-submit command).\n      Default: <empty string>\n    --sync-tool-classes\n      Meta sync client tool, using comma to separate multi tools\n      Default: org.apache.hudi.hive.HiveSyncTool\n  * --table-type\n      Type of table. COPY_ON_WRITE (or) MERGE_ON_READ\n  * --target-base-path\n      base path for the target hoodie table. (Will be created if did not exist\n      first time around. If exists, expected to be a hoodie table)\n  * --target-table\n      name of the target table\n    --transformer-class\n      A subclass or a list of subclasses of\n      org.apache.hudi.utilities.transform.Transformer. Allows transforming raw\n      source Dataset to a target Dataset (conforming to target schema) before\n      writing. Default : Not set. E.g. -\n      org.apache.hudi.utilities.transform.SqlQueryBasedTransformer (which\n      allows a SQL query templated to be passed as a transformation function).\n      Pass a comma-separated list of subclass names to chain the\n      transformations. If there are two or more transformers using the same\n      config keys and expect different values for those keys, then transformer\n      can include an identifier. E.g. -\n      tr1:org.apache.hudi.utilities.transform.SqlQueryBasedTransformer. Here\n      the identifier tr1 can be used along with property key like\n      `hoodie.streamer.transformer.sql.tr1` to identify properties related to\n      the transformer. So effective value for\n      `hoodie.streamer.transformer.sql` is determined by key\n      `hoodie.streamer.transformer.sql.tr1` for this transformer. If\n      identifier is used, it should be specified for all the transformers.\n      Further the order in which transformer is applied is determined by the\n      occurrence of transformer irrespective of the identifier used for the\n      transformer. For example: In the configured value below tr2:org.apache.hudi.utilities.transform.SqlQueryBasedTransformer,tr1:org.apache.hudi.utilities.transform.SqlQueryBasedTransformer\n      , tr2 is applied before tr1 based on order of occurrence.\n'))),(0,r.yg)("p",null,"The tool takes a hierarchically composed property file and has pluggable interfaces for extracting data, key generation and providing schema. Sample configs for ingesting from kafka and dfs are\nprovided under ",(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities/src/test/resources/streamer-config"),"."),(0,r.yg)("p",null,"For e.g: once you have Confluent Kafka, Schema registry up & running, produce some test data using (",(0,r.yg)("a",{parentName:"p",href:"https://docs.confluent.io/current/ksql/docs/tutorials/generate-custom-test-data"},"impressions.avro")," provided by schema-registry repo)"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"[confluent-5.0.0]$ bin/ksql-datagen schema=../impressions.avro format=avro topic=impressions key=impressionid\n")),(0,r.yg)("p",null,"and then ingest it as follows."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --target-base-path file:\\/\\/\\/tmp/hudi-streamer-op \\ \n  --target-table uber.impressions \\\n  --op BULK_INSERT\n")),(0,r.yg)("p",null,"In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to ",(0,r.yg)("a",{parentName:"p",href:"/docs/migration_guide"},"migration guide"),"."),(0,r.yg)("h3",{id:"using-hudi-utilities-bundle-jars"},"Using ",(0,r.yg)("inlineCode",{parentName:"h3"},"hudi-utilities")," bundle jars"),(0,r.yg)("p",null,"From 0.11.0 release, we start to provide a new ",(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities-slim-bundle")," which aims to exclude dependencies that can\ncause conflicts and compatibility issues with different versions of Spark."),(0,r.yg)("p",null,"It is recommended to switch to ",(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities-slim-bundle"),", which should be used along with a Hudi Spark bundle\ncorresponding the Spark version used to make utilities work with Spark, e.g.,\n",(0,r.yg)("inlineCode",{parentName:"p"},"--packages org.apache.hudi:hudi-utilities-slim-bundle_2.12:0.13.0,org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0"),"."),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities-bundle")," remains as a legacy bundle jar to work with Spark 2.4 and 3.1."),(0,r.yg)("h3",{id:"concurrency-control"},"Concurrency Control"),(0,r.yg)("p",null,"Using optimistic concurrency control (OCC) via Hudi Streamer requires the configs below to the properties file that can be passed to the\njob. "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-properties"},"hoodie.write.concurrency.mode=optimistic_concurrency_control\nhoodie.write.lock.provider=<lock-provider-classname>\nhoodie.cleaner.policy.failed.writes=LAZY\n")),(0,r.yg)("p",null,"As an example, adding the configs to ",(0,r.yg)("inlineCode",{parentName:"p"},"kafka-source.properties")," file and passing them to Hudi Streamer will enable OCC.\nA Hudi Streamer job can then be triggered as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --target-base-path file:///tmp/hudi-streamer-op \\ \n  --target-table uber.impressions \\\n  --op BULK_INSERT\n")),(0,r.yg)("p",null,"Read more in depth about concurrency control in the ",(0,r.yg)("a",{parentName:"p",href:"/docs/concurrency_control"},"concurrency control concepts")," section"),(0,r.yg)("h3",{id:"checkpointing"},"Checkpointing"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer")," uses checkpoints to keep track of what data has been read already so it can resume without needing to reprocess all data.\nWhen using a Kafka source, the checkpoint is the ",(0,r.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/KAFKA/Offset+Management"},"Kafka Offset"),"\nWhen using a DFS source, the checkpoint is the 'last modified' timestamp of the latest file read.\nCheckpoints are saved in the .hoodie commit file as ",(0,r.yg)("inlineCode",{parentName:"p"},"streamer.checkpoint.key"),"."),(0,r.yg)("p",null,"If you need to change the checkpoints for reprocessing or replaying data you can use the following options:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"--checkpoint")," will set ",(0,r.yg)("inlineCode",{parentName:"li"},"streamer.checkpoint.reset_key")," in the commit file to overwrite the current checkpoint."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"--source-limit")," will set a maximum amount of data to read from the source. For DFS sources, this is max # of bytes read.\nFor Kafka, this is the max # of events to read.")),(0,r.yg)("h3",{id:"transformers"},"Transformers"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer")," supports custom transformation on records before writing to storage. This is done by supplying\nimplementation of ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.transform.Transformer")," via ",(0,r.yg)("inlineCode",{parentName:"p"},"--transformer-class")," option. Check out\nthe ",(0,r.yg)("a",{parentName:"p",href:"#options"},"options")," section for details."),(0,r.yg)("h3",{id:"schema-providers"},"Schema Providers"),(0,r.yg)("p",null,"By default, Spark will infer the schema of the source and use that inferred schema when writing to a table. If you need\nto explicitly define the schema you can use one of the following Schema Providers below."),(0,r.yg)("h4",{id:"schema-registry-provider"},"Schema Registry Provider"),(0,r.yg)("p",null,"You can obtain the latest schema from an online registry. You pass a URL to the registry and if needed, you can also\npass userinfo and credentials in the url like: ",(0,r.yg)("inlineCode",{parentName:"p"},"https://foo:bar@schemaregistry.org")," The credentials are then extracted\nand are set on the request as an Authorization Header."),(0,r.yg)("p",null,"When fetching schemas from a registry, you can specify both the source schema and the target schema separately."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Config"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Example"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.registry.url"),(0,r.yg)("td",{parentName:"tr",align:null},"The schema of the source you are reading from"),(0,r.yg)("td",{parentName:"tr",align:null},"https://foo:",(0,r.yg)("a",{parentName:"td",href:"mailto:bar@schemaregistry.org"},"bar@schemaregistry.org"))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.registry.targetUrl"),(0,r.yg)("td",{parentName:"tr",align:null},"The schema of the target you are writing to"),(0,r.yg)("td",{parentName:"tr",align:null},"https://foo:",(0,r.yg)("a",{parentName:"td",href:"mailto:bar@schemaregistry.org"},"bar@schemaregistry.org"))))),(0,r.yg)("p",null,"The above configs are passed to Hudi Streamer spark-submit command like: "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"--hoodie-conf hoodie.streamer.schemaprovider.registry.url=https://foo:bar@schemaregistry.org\n")),(0,r.yg)("p",null,"There are other optional configs to work with schema registry provider such as SSL-store related configs, and supporting\ncustom transformation of schema returned by schema registry, e.g., converting the original json schema to avro schema\nvia ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.schema.converter.JsonToAvroSchemaConverter"),"."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Config"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Example"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.registry.schemaconverter"),(0,r.yg)("td",{parentName:"tr",align:null},"The class name of the custom schema converter to use"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"org.apache.hudi.utilities.schema.converter.JsonToAvroSchemaConverter"))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"schema.registry.ssl.keystore.location"),(0,r.yg)("td",{parentName:"tr",align:null},"SSL key store location"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"schema.registry.ssl.keystore.password"),(0,r.yg)("td",{parentName:"tr",align:null},"SSL key store password"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"schema.registry.ssl.truststore.location"),(0,r.yg)("td",{parentName:"tr",align:null},"SSL trust store location"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"schema.registry.ssl.truststore.password"),(0,r.yg)("td",{parentName:"tr",align:null},"SSL trust store password"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"schema.registry.ssl.key.password"),(0,r.yg)("td",{parentName:"tr",align:null},"SSL key password"),(0,r.yg)("td",{parentName:"tr",align:null})))),(0,r.yg)("h4",{id:"jdbc-schema-provider"},"JDBC Schema Provider"),(0,r.yg)("p",null,"You can obtain the latest schema through a JDBC connection."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Config"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Example"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.connection.url"),(0,r.yg)("td",{parentName:"tr",align:null},"The JDBC URL to connect to. You can specify source specific connection properties in the URL"),(0,r.yg)("td",{parentName:"tr",align:null},"jdbc:postgresql://localhost/test?user=fred&password=secret")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.driver.type"),(0,r.yg)("td",{parentName:"tr",align:null},"The class name of the JDBC driver to use to connect to this URL"),(0,r.yg)("td",{parentName:"tr",align:null},"org.h2.Driver")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.username"),(0,r.yg)("td",{parentName:"tr",align:null},"username for the connection"),(0,r.yg)("td",{parentName:"tr",align:null},"fred")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.password"),(0,r.yg)("td",{parentName:"tr",align:null},"password for the connection"),(0,r.yg)("td",{parentName:"tr",align:null},"secret")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.dbtable"),(0,r.yg)("td",{parentName:"tr",align:null},"The table with the schema to reference"),(0,r.yg)("td",{parentName:"tr",align:null},"test_database.test1_table or test1_table")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.timeout"),(0,r.yg)("td",{parentName:"tr",align:null},"The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to 0."),(0,r.yg)("td",{parentName:"tr",align:null},"0")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.jdbc.nullable"),(0,r.yg)("td",{parentName:"tr",align:null},"If true, all columns are nullable"),(0,r.yg)("td",{parentName:"tr",align:null},"true")))),(0,r.yg)("p",null,"The above configs are passed to Hudi Streamer spark-submit command like:\n",(0,r.yg)("inlineCode",{parentName:"p"},"--hoodie-conf hoodie.streamer.jdbcbasedschemaprovider.connection.url=jdbc:postgresql://localhost/test?user=fred&password=secret")),(0,r.yg)("h4",{id:"file-based-schema-provider"},"File Based Schema Provider"),(0,r.yg)("p",null,"You can use a .avsc file to define your schema. You can then point to this file on DFS as a schema provider."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Config"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Example"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.file"),(0,r.yg)("td",{parentName:"tr",align:null},"The schema of the source you are reading from"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("a",{parentName:"td",href:"https://github.com/apache/hudi/blob/a8fb69656f522648233f0310ca3756188d954281/docker/demo/config/test-suite/source.avsc"},"example schema file"))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.target.schema.file"),(0,r.yg)("td",{parentName:"tr",align:null},"The schema of the target you are writing to"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("a",{parentName:"td",href:"https://github.com/apache/hudi/blob/a8fb69656f522648233f0310ca3756188d954281/docker/demo/config/test-suite/target.avsc"},"example schema file"))))),(0,r.yg)("h4",{id:"hive-schema-provider"},"Hive Schema Provider"),(0,r.yg)("p",null,"You can use hive tables to fetch source and target schema. "),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Config"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.hive.database"),(0,r.yg)("td",{parentName:"tr",align:null},"Hive database from where source schema can be fetched")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.source.schema.hive.table"),(0,r.yg)("td",{parentName:"tr",align:null},"Hive table from where source schema can be fetched")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.target.schema.hive.database"),(0,r.yg)("td",{parentName:"tr",align:null},"Hive database from where target schema can be fetched")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.schemaprovider.target.schema.hive.table"),(0,r.yg)("td",{parentName:"tr",align:null},"Hive table from where target schema can be fetched")))),(0,r.yg)("h4",{id:"schema-provider-with-post-processor"},"Schema Provider with Post Processor"),(0,r.yg)("p",null,"The SchemaProviderWithPostProcessor, will extract the schema from one of the previously mentioned Schema Providers and\nthen will apply a post processor to change the schema before it is used. You can write your own post processor by extending\nthis class: ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaPostProcessor.java"},"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaPostProcessor.java")),(0,r.yg)("h3",{id:"sources"},"Sources"),(0,r.yg)("p",null,"Hoodie Streamer can read data from a wide variety of sources. The following are a list of supported sources:"),(0,r.yg)("h4",{id:"distributed-file-system-dfs"},"Distributed File System (DFS)"),(0,r.yg)("p",null,"See the storage configurations page to see some examples of DFS applications Hudi can read from. The following are the\nsupported file formats Hudi can read/write with on DFS Sources. (Note: you can still use Spark/Flink readers to read from\nother formats and then write data as Hudi format.)"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"CSV"),(0,r.yg)("li",{parentName:"ul"},"AVRO"),(0,r.yg)("li",{parentName:"ul"},"JSON"),(0,r.yg)("li",{parentName:"ul"},"PARQUET"),(0,r.yg)("li",{parentName:"ul"},"ORC"),(0,r.yg)("li",{parentName:"ul"},"HUDI")),(0,r.yg)("p",null,"For DFS sources the following behaviors are expected:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"For JSON DFS source, you always need to set a schema. If the target Hudi table follows the same schema as from the source file, you just need to set the source schema. If not, you need to set schemas for both source and target. "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"HoodieStreamer")," reads the files under the source base path (",(0,r.yg)("inlineCode",{parentName:"li"},"hoodie.streamer.source.dfs.root"),") directly, and it won't use the partition paths under this base path as fields of the dataset. Detailed examples can be found ",(0,r.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/issues/5485"},"here"),".")),(0,r.yg)("h4",{id:"kafka"},"Kafka"),(0,r.yg)("p",null,"Hudi can read directly from Kafka clusters. See more details on ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer")," to learn how to setup streaming\ningestion with exactly once semantics, checkpointing, and plugin transformations. The following formats are supported\nwhen reading data from Kafka:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"AVRO: ",(0,r.yg)("inlineCode",{parentName:"li"},"org.apache.hudi.utilities.sources.AvroKafkaSource")),(0,r.yg)("li",{parentName:"ul"},"JSON: ",(0,r.yg)("inlineCode",{parentName:"li"},"org.apache.hudi.utilities.sources.JsonKafkaSource")),(0,r.yg)("li",{parentName:"ul"},"Proto: ",(0,r.yg)("inlineCode",{parentName:"li"},"org.apache.hudi.utilities.sources.ProtoKafkaSource"))),(0,r.yg)("p",null,"Check out ",(0,r.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#Kafka-Source-Configs"},"Kafka source config")," for more details."),(0,r.yg)("h4",{id:"pulsar"},"Pulsar"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer")," also supports ingesting from Apache Pulsar via ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.sources.PulsarSource"),".\nCheck out ",(0,r.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#Pulsar-Source-Configs"},"Pulsar source config")," for more details."),(0,r.yg)("h4",{id:"cloud-storage-event-sources"},"Cloud storage event sources"),(0,r.yg)("p",null,"AWS S3 storage provides an event notification service which will post notifications when certain events happen in your S3 bucket:\n",(0,r.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"},"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"),"\nAWS will put these events in a Simple Queue Service (SQS). Apache Hudi provides ",(0,r.yg)("inlineCode",{parentName:"p"},"S3EventsSource"),"\nand ",(0,r.yg)("inlineCode",{parentName:"p"},"S3EventsHoodieIncrSource")," that can read from SQS to trigger/processing of new or changed data as soon as it is\navailable on S3. Check out ",(0,r.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#S3-Source-Configs"},"S3 source configs")," for more details."),(0,r.yg)("p",null,"Similar to S3 event source, Google Cloud Storage (GCS) event source is also supported via ",(0,r.yg)("inlineCode",{parentName:"p"},"GcsEventsSource")," and\n",(0,r.yg)("inlineCode",{parentName:"p"},"GcsEventsHoodieIncrSource"),". Check out ",(0,r.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#GCS-Events-Source-Configs"},"GCS events source configs")," for more details."),(0,r.yg)("h5",{id:"aws-setup"},"AWS Setup"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Enable S3 Event Notifications ",(0,r.yg)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"},"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html")),(0,r.yg)("li",{parentName:"ol"},"Download the aws-java-sdk-sqs jar. "),(0,r.yg)("li",{parentName:"ol"},"Find the queue URL and Region to set these configurations:",(0,r.yg)("ol",{parentName:"li"},(0,r.yg)("li",{parentName:"ol"},"hoodie.streamer.s3.source.queue.url=",(0,r.yg)("a",{parentName:"li",href:"https://sqs.us-west-2.amazonaws.com/queue/url"},"https://sqs.us-west-2.amazonaws.com/queue/url")),(0,r.yg)("li",{parentName:"ol"},"hoodie.streamer.s3.source.queue.region=us-west-2 "))),(0,r.yg)("li",{parentName:"ol"},"Start the ",(0,r.yg)("inlineCode",{parentName:"li"},"S3EventsSource")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"S3EventsHoodieIncrSource")," using the ",(0,r.yg)("inlineCode",{parentName:"li"},"HoodieStreamer")," utility as shown in sample commands below:")),(0,r.yg)("p",null,"Insert code sample from this blog: ",(0,r.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/08/23/s3-events-source/#configuration-and-setup"},"https://hudi.apache.org/blog/2021/08/23/s3-events-source/#configuration-and-setup")),(0,r.yg)("h4",{id:"jdbc-source"},"JDBC Source"),(0,r.yg)("p",null,"Hudi can read from a JDBC source with a full fetch of a table, or Hudi can even read incrementally with checkpointing from a JDBC source."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Config"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Example"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.url"),(0,r.yg)("td",{parentName:"tr",align:null},"URL of the JDBC connection"),(0,r.yg)("td",{parentName:"tr",align:null},"jdbc:postgresql://localhost/test")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.user"),(0,r.yg)("td",{parentName:"tr",align:null},"User to use for authentication of the JDBC connection"),(0,r.yg)("td",{parentName:"tr",align:null},"fred")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.password"),(0,r.yg)("td",{parentName:"tr",align:null},"Password to use for authentication of the JDBC connection"),(0,r.yg)("td",{parentName:"tr",align:null},"secret")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.password.file"),(0,r.yg)("td",{parentName:"tr",align:null},"If you prefer to use a password file for the connection"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.driver.class"),(0,r.yg)("td",{parentName:"tr",align:null},"Driver class to use for the JDBC connection"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.table.name"),(0,r.yg)("td",{parentName:"tr",align:null}),(0,r.yg)("td",{parentName:"tr",align:null},"my_table")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.table.incr.column.name"),(0,r.yg)("td",{parentName:"tr",align:null},"If run in incremental mode, this field will be used to pull new data incrementally"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.incr.pull"),(0,r.yg)("td",{parentName:"tr",align:null},"Will the JDBC connection perform an incremental pull?"),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.extra.options."),(0,r.yg)("td",{parentName:"tr",align:null},"How you pass extra configurations that would normally by specified as spark.read.option()"),(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.extra.options.fetchSize=100 hoodie.streamer.jdbc.extra.options.upperBound=1 hoodie.streamer.jdbc.extra.options.lowerBound=100")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.storage.level"),(0,r.yg)("td",{parentName:"tr",align:null},"Used to control the persistence level"),(0,r.yg)("td",{parentName:"tr",align:null},"Default = MEMORY_AND_DISK_SER")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"hoodie.streamer.jdbc.incr.fallback.to.full.fetch"),(0,r.yg)("td",{parentName:"tr",align:null},"Boolean which if set true makes an incremental fetch fallback to a full fetch if there is any error in the incremental read"),(0,r.yg)("td",{parentName:"tr",align:null},"FALSE")))),(0,r.yg)("h4",{id:"sql-sources"},"SQL Sources"),(0,r.yg)("p",null,"SQL Source ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.sources.SqlSource")," reads from any table, used mainly for backfill jobs which will process specific partition dates.\nThis won't update the streamer.checkpoint.key to the processed commit, instead it will fetch the latest successful\ncheckpoint key and set that value as this backfill commits checkpoint so that it won't interrupt the regular incremental\nprocessing. To fetch and use the latest incremental checkpoint, you need to also set this hoodie_conf for Hudi Streamer\njobs: ",(0,r.yg)("inlineCode",{parentName:"p"},"hoodie.write.meta.key.prefixes = 'streamer.checkpoint.key'")),(0,r.yg)("p",null,"Spark SQL should be configured using this hoodie config:\n",(0,r.yg)("inlineCode",{parentName:"p"},"hoodie.streamer.source.sql.sql.query = 'select * from source_table'")),(0,r.yg)("p",null,"Using ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.sources.SqlFileBasedSource")," allows setting the SQL queries in a file to read from any\ntable. SQL file path should be configured using this hoodie config:\n",(0,r.yg)("inlineCode",{parentName:"p"},"hoodie.streamer.source.sql.file = 'hdfs://xxx/source.sql'")),(0,r.yg)("h3",{id:"error-table"},"Error Table"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer"),' supports segregating error records into a separate table called "Error table" alongside with the\ntarget data table. This allows easy integration with dead-letter queues (DLQ). Error Table is supported with\nuser-provided subclass of ',(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.streamer.BaseErrorTableWriter")," supplied via\nconfig ",(0,r.yg)("inlineCode",{parentName:"p"},"hoodie.errortable.write.class"),". Check out more in ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.config.HoodieErrorTableConfig"),"."),(0,r.yg)("h3",{id:"termination-strategy"},"Termination Strategy"),(0,r.yg)("p",null,"Users can configure a post-write termination strategy under ",(0,r.yg)("inlineCode",{parentName:"p"},"continuous")," mode if need be. For instance,\nusers can configure graceful shutdown if there is no new data from the configured source for 5 consecutive times.\nHere is the interface for the termination strategy."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"/**\n * Post write termination strategy for deltastreamer in continuous mode.\n */\npublic interface PostWriteTerminationStrategy {\n\n  /**\n   * Returns whether HoodieStreamer needs to be shutdown.\n   * @param scheduledCompactionInstantAndWriteStatuses optional pair of scheduled compaction instant and write statuses.\n   * @return true if HoodieStreamer has to be shutdown. false otherwise.\n   */\n  boolean shouldShutdown(Option<Pair<Option<String>, JavaRDD<WriteStatus>>> scheduledCompactionInstantAndWriteStatuses);\n\n}\n")),(0,r.yg)("p",null,"Also, this might help in bootstrapping a new table. Instead of doing one bulk load or bulk_insert leveraging a large\ncluster for a large input of data, one could start ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer")," on the ",(0,r.yg)("inlineCode",{parentName:"p"},"continuous")," mode and add a shutdown strategy\nto terminate, once all data has been bootstrapped. This way, each batch could be smaller and may not need a large\ncluster to bootstrap data. There is a concrete implementation provided out-of-the-box: ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/0d0a4152cfd362185066519ae926ac4513c7a152/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/NoNewDataTerminationStrategy.java"},"NoNewDataTerminationStrategy"),".\nUsers can feel free to implement their own strategy as they see fit."),(0,r.yg)("h3",{id:"dynamic-configuration-updates"},"Dynamic configuration updates"),(0,r.yg)("p",null,"When Hoodie Streamer is running in ",(0,r.yg)("inlineCode",{parentName:"p"},"continuous")," mode, the properties can be refreshed/updated before each sync calls.\nInterested users can implement ",(0,r.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.deltastreamer.ConfigurationHotUpdateStrategy")," to leverage this."),(0,r.yg)("h2",{id:"multitablestreamer"},"MultiTableStreamer"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableStreamer"),", an extension of ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer"),", facilitates the simultaneous ingestion of multiple tables into Hudi datasets. At present, it supports the sequential ingestion of tables and accommodates both COPY_ON_WRITE and MERGE_ON_READ storage types. The command line parameters for ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableStreamer")," largely mirror those of ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer"),", with the notable difference being the necessity to supply table-specific configurations in separate files in a dedicated config folder. New command line options have been introduced to support this functionality:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"  * --config-folder\n    the path to the folder which contains all the table wise config files\n    --base-path-prefix\n    this is added to enable users to create all the hudi datasets for related tables under one path in FS. The datasets are then created under the path - <base_path_prefix>/<database>/<table_to_be_ingested>. However you can override the paths for every table by setting the property hoodie.streamer.ingestion.targetBasePath\n")),(0,r.yg)("p",null,"The following properties are needed to be set properly to ingest data using ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableStreamer"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.streamer.ingestion.tablesToBeIngested\n  comma separated names of tables to be ingested in the format <database>.<table>, for example db1.table1,db1.table2\nhoodie.streamer.ingestion.targetBasePath\n  if you wish to ingest a particular table in a separate path, you can mention that path here\nhoodie.streamer.ingestion.<database>.<table>.configFile\n  path to the config file in dedicated config folder which contains table overridden properties for the particular table to be ingested.\n")),(0,r.yg)("p",null,"Sample config files for table wise overridden properties can be found\nunder ",(0,r.yg)("inlineCode",{parentName:"p"},"hudi-utilities/src/test/resources/streamer-config"),". The command to run ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableStreamer")," is also similar\nto how you run ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieMultiTableStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \\\n  --config-folder file://tmp/hudi-ingestion-config \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --base-path-prefix file:\\/\\/\\/tmp/hudi-streamer-op \\ \n  --target-table uber.impressions \\\n  --op BULK_INSERT\n")),(0,r.yg)("p",null,"For detailed information on how to configure and use ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableStreamer"),", please refer ",(0,r.yg)("a",{parentName:"p",href:"/blog/2020/08/22/ingest-multiple-tables-using-hudi"},"blog section"),"."),(0,r.yg)("h2",{id:"structured-streaming"},"Structured Streaming"),(0,r.yg)("p",null,"Hudi supports Spark Structured Streaming reads and writes."),(0,r.yg)("h3",{id:"streaming-write"},"Streaming Write"),(0,r.yg)("p",null,"You can write Hudi tables using spark's structured streaming. "),(0,r.yg)(i.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],mdxType:"Tabs"},(0,r.yg)(o.A,{value:"scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'// spark-shell\n// prepare to stream write to new table\nimport org.apache.spark.sql.streaming.Trigger\n\nval streamingTableName = "hudi_trips_cow_streaming"\nval baseStreamingPath = "file:///tmp/hudi_trips_cow_streaming"\nval checkpointLocation = "file:///tmp/checkpoints/hudi_trips_cow_streaming"\n\n// create streaming df\nval df = spark.readStream.\n        format("hudi").\n        load(basePath)\n\n// write stream to new hudi table\ndf.writeStream.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, streamingTableName).\n  outputMode("append").\n  option("path", baseStreamingPath).\n  option("checkpointLocation", checkpointLocation).\n  trigger(Trigger.Once()).\n  start()\n\n'))),(0,r.yg)(o.A,{value:"python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"# pyspark\n# prepare to stream write to new table\nstreamingTableName = \"hudi_trips_cow_streaming\"\nbaseStreamingPath = \"file:///tmp/hudi_trips_cow_streaming\"\ncheckpointLocation = \"file:///tmp/checkpoints/hudi_trips_cow_streaming\"\n\nhudi_streaming_options = {\n    'hoodie.table.name': streamingTableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': streamingTableName,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\n\n# create streaming df\ndf = spark.readStream \\\n    .format(\"hudi\") \\\n    .load(basePath)\n\n# write stream to new hudi table\ndf.writeStream.format(\"hudi\") \\\n    .options(**hudi_streaming_options) \\\n    .outputMode(\"append\") \\\n    .option(\"path\", baseStreamingPath) \\\n    .option(\"checkpointLocation\", checkpointLocation) \\\n    .trigger(once=True) \\\n    .start()\n\n")))),(0,r.yg)("h3",{id:"streaming-read"},"Streaming Read"),(0,r.yg)("p",null,"Structured Streaming reads are based on Hudi's Incremental Query feature, therefore streaming read can return data for which\ncommits and base files were not yet removed by the cleaner. You can control commits retention time."),(0,r.yg)(i.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],mdxType:"Tabs"},(0,r.yg)(o.A,{value:"scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'// spark-shell\n// reload data\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath)\n\n// read stream and output results to console\nspark.readStream.\n  format("hudi").\n  load(basePath).\n  writeStream.\n  format("console").\n  start()\n\n// read stream to streaming df\nval df = spark.readStream.\n        format("hudi").\n        load(basePath)\n\n'))),(0,r.yg)(o.A,{value:"python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"# pyspark\n# reload data\ninserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(\n    dataGen.generateInserts(10))\ndf = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n\nhudi_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': tableName,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\n\ndf.write.format(\"hudi\"). \\\n    options(**hudi_options). \\\n    mode(\"overwrite\"). \\\n    save(basePath)\n\n# read stream to streaming df\ndf = spark.readStream \\\n    .format(\"hudi\") \\\n    .load(basePath)\n\n# ead stream and output results to console\nspark.readStream \\\n    .format(\"hudi\") \\\n    .load(basePath) \\\n    .writeStream \\\n    .format(\"console\") \\\n    .start()\n\n")))),(0,r.yg)("div",{className:"admonition admonition-info alert alert--info"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"Spark SQL can be used within ForeachBatch sink to do INSERT, UPDATE, DELETE and MERGE INTO.\nTarget table must exist before write."))),(0,r.yg)("h2",{id:"flink-ingestion"},"Flink Ingestion"),(0,r.yg)("h3",{id:"cdc-ingestion"},"CDC Ingestion"),(0,r.yg)("p",null,"CDC(change data capture) keep track of the data changes evolving in a source system so a downstream process or system can action that change.\nWe recommend two ways for syncing CDC data into Hudi:"),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"slide1 title",src:a(45859).A})),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Using the Ververica ",(0,r.yg)("a",{parentName:"li",href:"https://github.com/ververica/flink-cdc-connectors"},"flink-cdc-connectors")," directly connect to DB Server to sync the binlog data into Hudi.\nThe advantage is that it does not rely on message queues, but the disadvantage is that it puts pressure on the db server;"),(0,r.yg)("li",{parentName:"ol"},"Consume data from a message queue (for e.g, the Kafka) using the flink cdc format, the advantage is that it is highly scalable,\nbut the disadvantage is that it relies on message queues.")),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("ul",{parentName:"div"},(0,r.yg)("li",{parentName:"ul"},"If the upstream data cannot guarantee the order, you need to specify option ",(0,r.yg)("inlineCode",{parentName:"li"},"write.precombine.field")," explicitly;")))),(0,r.yg)("h3",{id:"bulk-insert"},"Bulk Insert"),(0,r.yg)("p",null,"For the demand of snapshot data import. If the snapshot data comes from other data sources, use the ",(0,r.yg)("inlineCode",{parentName:"p"},"bulk_insert")," mode to quickly\nimport the snapshot data into Hudi."),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},(0,r.yg)("inlineCode",{parentName:"p"},"bulk_insert")," eliminates the serialization and data merging. The data deduplication is skipped, so the user need to guarantee the uniqueness of the data."))),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},(0,r.yg)("inlineCode",{parentName:"p"},"bulk_insert")," is more efficient in the ",(0,r.yg)("inlineCode",{parentName:"p"},"batch execution mode"),". By default, the ",(0,r.yg)("inlineCode",{parentName:"p"},"batch execution mode")," sorts the input records\nby the partition path and writes these records to Hudi, which can avoid write performance degradation caused by\nfrequent ",(0,r.yg)("inlineCode",{parentName:"p"},"file handle")," switching.  "))),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"The parallelism of ",(0,r.yg)("inlineCode",{parentName:"p"},"bulk_insert")," is specified by ",(0,r.yg)("inlineCode",{parentName:"p"},"write.tasks"),". The parallelism will affect the number of small files.\nIn theory, the parallelism of ",(0,r.yg)("inlineCode",{parentName:"p"},"bulk_insert")," is the number of ",(0,r.yg)("inlineCode",{parentName:"p"},"bucket"),"s (In particular, when each bucket writes to maximum file size, it\nwill rollover to the new file handle. Finally, ",(0,r.yg)("inlineCode",{parentName:"p"},"the number of files")," >= ",(0,r.yg)("a",{parentName:"p",href:"/docs/configurations#writebucket_assigntasks"},(0,r.yg)("inlineCode",{parentName:"a"},"write.bucket_assign.tasks")),"."))),(0,r.yg)("h4",{id:"options-1"},"Options"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.operation")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"true")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"upsert")),(0,r.yg)("td",{parentName:"tr",align:null},"Setting as ",(0,r.yg)("inlineCode",{parentName:"td"},"bulk_insert")," to open this function")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.tasks")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"4")),(0,r.yg)("td",{parentName:"tr",align:null},"The parallelism of ",(0,r.yg)("inlineCode",{parentName:"td"},"bulk_insert"),", ",(0,r.yg)("inlineCode",{parentName:"td"},"the number of files")," >= ",(0,r.yg)("a",{parentName:"td",href:"/docs/configurations#writebucket_assigntasks"},(0,r.yg)("inlineCode",{parentName:"a"},"write.bucket_assign.tasks")))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.bulk_insert.shuffle_input")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"true")),(0,r.yg)("td",{parentName:"tr",align:null},"Whether to shuffle data according to the input field before writing. Enabling this option will reduce the number of small files, but there may be a risk of data skew")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.bulk_insert.sort_input")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"true")),(0,r.yg)("td",{parentName:"tr",align:null},"Whether to sort data according to the input field before writing. Enabling this option will reduce the number of small files when a write task writes multiple partitions")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.sort.memory")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"128")),(0,r.yg)("td",{parentName:"tr",align:null},"Available managed memory of sort operator. default  ",(0,r.yg)("inlineCode",{parentName:"td"},"128")," MB")))),(0,r.yg)("h3",{id:"index-bootstrap"},"Index Bootstrap"),(0,r.yg)("p",null,"For the demand of ",(0,r.yg)("inlineCode",{parentName:"p"},"snapshot data")," + ",(0,r.yg)("inlineCode",{parentName:"p"},"incremental data")," import. If the ",(0,r.yg)("inlineCode",{parentName:"p"},"snapshot data")," already insert into Hudi by  ",(0,r.yg)("a",{parentName:"p",href:"#bulk-insert"},"bulk insert"),".\nUser can insert ",(0,r.yg)("inlineCode",{parentName:"p"},"incremental data")," in real time and ensure the data is not repeated by using the index bootstrap function."),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"If you think this process is very time-consuming, you can add resources to write in streaming mode while writing ",(0,r.yg)("inlineCode",{parentName:"p"},"snapshot data"),",\nand then reduce the resources to write ",(0,r.yg)("inlineCode",{parentName:"p"},"incremental data")," (or open the rate limit function)."))),(0,r.yg)("h4",{id:"options-2"},"Options"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"index.bootstrap.enabled")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"true")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},"When index bootstrap is enabled, the remain records in Hudi table will be loaded into the Flink state at one time")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"index.partition.regex")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"*")),(0,r.yg)("td",{parentName:"tr",align:null},"Optimize option. Setting regular expressions to filter partitions. By default, all partitions are loaded into flink state")))),(0,r.yg)("h4",{id:"how-to-use"},"How To Use"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"CREATE TABLE")," creates a statement corresponding to the Hudi table. Note that the ",(0,r.yg)("inlineCode",{parentName:"li"},"table.type")," must be correct."),(0,r.yg)("li",{parentName:"ol"},"Setting ",(0,r.yg)("inlineCode",{parentName:"li"},"index.bootstrap.enabled")," = ",(0,r.yg)("inlineCode",{parentName:"li"},"true")," to enable the index bootstrap function."),(0,r.yg)("li",{parentName:"ol"},"Setting Flink checkpoint failure tolerance in ",(0,r.yg)("inlineCode",{parentName:"li"},"flink-conf.yaml")," : ",(0,r.yg)("inlineCode",{parentName:"li"},"execution.checkpointing.tolerable-failed-checkpoints = n")," (depending on Flink checkpoint scheduling times)."),(0,r.yg)("li",{parentName:"ol"},"Waiting until the first checkpoint succeeds, indicating that the index bootstrap completed."),(0,r.yg)("li",{parentName:"ol"},"After the index bootstrap completed, user can exit and save the savepoint (or directly use the externalized checkpoint)."),(0,r.yg)("li",{parentName:"ol"},"Restart the job, setting ",(0,r.yg)("inlineCode",{parentName:"li"},"index.bootstrap.enable")," as ",(0,r.yg)("inlineCode",{parentName:"li"},"false"),".")),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("ol",{parentName:"div"},(0,r.yg)("li",{parentName:"ol"},"Index bootstrap is blocking, so checkpoint cannot be completed during index bootstrap."),(0,r.yg)("li",{parentName:"ol"},"Index bootstrap triggers by the input data. User need to ensure that there is at least one record in each partition."),(0,r.yg)("li",{parentName:"ol"},"Index bootstrap executes concurrently. User can search in log by ",(0,r.yg)("inlineCode",{parentName:"li"},"finish loading the index under partition")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"Load record form file")," to observe the progress of index bootstrap."),(0,r.yg)("li",{parentName:"ol"},"The first successful checkpoint indicates that the index bootstrap completed. There is no need to load the index again when recovering from the checkpoint.")))),(0,r.yg)("h3",{id:"changelog-mode"},"Changelog Mode"),(0,r.yg)("p",null,"Hudi can keep all the intermediate changes (I / -U / U / D) of messages, then consumes through stateful computing of flink to have a near-real-time\ndata warehouse ETL pipeline (Incremental computing). Hudi MOR table stores messages in the forms of rows, which supports the retention of all change logs (Integration at the format level).\nAll changelog records can be consumed with Flink streaming reader."),(0,r.yg)("h4",{id:"options-3"},"Options"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"changelog.enabled")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},"It is turned off by default, to have the ",(0,r.yg)("inlineCode",{parentName:"td"},"upsert")," semantics, only the merged messages are ensured to be kept, intermediate changes may be merged. Setting to true to support consumption of all changes")))),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"Batch (Snapshot) read still merge all the intermediate changes, regardless of whether the format has stored the intermediate changelog messages."))),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"After setting ",(0,r.yg)("inlineCode",{parentName:"p"},"changelog.enable")," as ",(0,r.yg)("inlineCode",{parentName:"p"},"true"),", the retention of changelog records are only best effort: the asynchronous compaction task will merge the changelog records into one record, so if the\nstream source does not consume timely, only the merged record for each key can be read after compaction. The solution is to reserve some buffer time for the reader by adjusting the compaction strategy, such as\nthe compaction options: ",(0,r.yg)("a",{parentName:"p",href:"#compaction"},(0,r.yg)("inlineCode",{parentName:"a"},"compaction.delta_commits"))," and ",(0,r.yg)("a",{parentName:"p",href:"#compaction"},(0,r.yg)("inlineCode",{parentName:"a"},"compaction.delta_seconds")),"."))),(0,r.yg)("h3",{id:"append-mode"},"Append Mode"),(0,r.yg)("p",null,"For ",(0,r.yg)("inlineCode",{parentName:"p"},"INSERT")," mode write operation, the current work flow is:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"For Merge_On_Read table, the small file strategies are by default applied: tries to append to the small avro log files first"),(0,r.yg)("li",{parentName:"ul"},"For Copy_On_Write table, write new parquet files directly, no small file strategies are applied")),(0,r.yg)("p",null,"Hudi supports rich clustering strategies to optimize the files layout for ",(0,r.yg)("inlineCode",{parentName:"p"},"INSERT")," mode:"),(0,r.yg)("h4",{id:"inline-clustering"},"Inline Clustering"),(0,r.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.yg)("div",{parentName:"div",className:"admonition-heading"},(0,r.yg)("h5",{parentName:"div"},(0,r.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,r.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.yg)("div",{parentName:"div",className:"admonition-content"},(0,r.yg)("p",{parentName:"div"},"Only Copy_On_Write table is supported. "))),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.insert.cluster")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},"Whether to merge small files while ingesting, for COW table, open the option to enable the small file merging strategy(no deduplication for keys but the throughput will be affected)")))),(0,r.yg)("h4",{id:"async-clustering"},"Async Clustering"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.schedule.enabled")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},"Whether to schedule clustering plan during write process, by default false")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.delta_commits")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"4")),(0,r.yg)("td",{parentName:"tr",align:null},"Delta commits to schedule the clustering plan, only valid when ",(0,r.yg)("inlineCode",{parentName:"td"},"clustering.schedule.enabled")," is true")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.async.enabled")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},"Whether to execute clustering plan asynchronously, by default false")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.tasks")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"4")),(0,r.yg)("td",{parentName:"tr",align:null},"Parallelism of the clustering tasks")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.target.file.max.bytes")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"1024*1024*1024")),(0,r.yg)("td",{parentName:"tr",align:null},"The target file size for clustering group, by default 1GB")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.small.file.limit")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"600")),(0,r.yg)("td",{parentName:"tr",align:null},"The file that has less size than the threshold (unit MB) are candidates for clustering")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.sort.columns")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"N/A")),(0,r.yg)("td",{parentName:"tr",align:null},"The columns to sort by when clustering")))),(0,r.yg)("h4",{id:"clustering-plan-strategy"},"Clustering Plan Strategy"),(0,r.yg)("p",null,"Custom clustering strategy is supported."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.partition.filter.mode")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"NONE")),(0,r.yg)("td",{parentName:"tr",align:null},"Valid options 1) ",(0,r.yg)("inlineCode",{parentName:"td"},"NONE"),": no limit; 2) ",(0,r.yg)("inlineCode",{parentName:"td"},"RECENT_DAYS"),": choose partitions that represent recent days; 3) ",(0,r.yg)("inlineCode",{parentName:"td"},"SELECTED_PARTITIONS"),": specific partitions")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.daybased.lookback.partitions")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"2")),(0,r.yg)("td",{parentName:"tr",align:null},"Valid for ",(0,r.yg)("inlineCode",{parentName:"td"},"RECENT_DAYS")," mode")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.cluster.begin.partition")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"N/A")),(0,r.yg)("td",{parentName:"tr",align:null},"Valid for ",(0,r.yg)("inlineCode",{parentName:"td"},"SELECTED_PARTITIONS")," mode, specify the partition to begin with(inclusive)")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.cluster.end.partition")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"N/A")),(0,r.yg)("td",{parentName:"tr",align:null},"Valid for ",(0,r.yg)("inlineCode",{parentName:"td"},"SELECTED_PARTITIONS")," mode, specify the partition to end with(inclusive)")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.partition.regex.pattern")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"N/A")),(0,r.yg)("td",{parentName:"tr",align:null},"The regex to filter the partitions")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"clustering.plan.strategy.partition.selected")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"N/A")),(0,r.yg)("td",{parentName:"tr",align:null},"Specific partitions separated by comma ",(0,r.yg)("inlineCode",{parentName:"td"},","))))),(0,r.yg)("h3",{id:"bucket-index"},"Bucket Index"),(0,r.yg)("p",null,"By default, flink uses the state-backend to keep the file index: the mapping from primary key to fileId. When the input data set is large,\nthere is possibility the cost of the state be a bottleneck, the bucket index use deterministic hash algorithm for shuffling the records into\nbuckets, thus can avoid the storage and query overhead of indexes."),(0,r.yg)("h4",{id:"options-4"},"Options"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"index.type")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"FLINK_STATE")),(0,r.yg)("td",{parentName:"tr",align:null},"Set up as ",(0,r.yg)("inlineCode",{parentName:"td"},"BUCKET")," to use bucket index")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"hoodie.bucket.index.hash.field")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},"Primary key"),(0,r.yg)("td",{parentName:"tr",align:null},"Can be a subset of the primary key")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"hoodie.bucket.index.num.buckets")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"4")),(0,r.yg)("td",{parentName:"tr",align:null},"The number of buckets per-partition, it is immutable once set up")))),(0,r.yg)("p",null,"Comparing to state index:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Bucket index has no computing and storage cost of state-backend index, thus has better performance"),(0,r.yg)("li",{parentName:"ul"},"Bucket index can not expand the buckets dynamically, the state-backend index can expand the buckets dynamically based on current file layout"),(0,r.yg)("li",{parentName:"ul"},"Bucket index can not handle changes among partitions(no limit if the input itself is CDC stream), state-backend index has no limit ")),(0,r.yg)("h3",{id:"rate-limit"},"Rate Limit"),(0,r.yg)("p",null,"There are many use cases that user put the full history data set onto the message queue together with the realtime incremental data. Then they consume the data from the queue into the hudi from the earliest offset using flink. Consuming history data set has these characteristics:\n1). The instant throughput is huge 2). It has serious disorder (with random writing partitions). It will lead to degradation of writing performance and throughput glitches. For this case, the speed limit parameter can be turned on to ensure smooth writing of the flow."),(0,r.yg)("h4",{id:"options-5"},"Options"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Option Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Required"),(0,r.yg)("th",{parentName:"tr",align:null},"Default"),(0,r.yg)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"write.rate.limit")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"false")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"0")),(0,r.yg)("td",{parentName:"tr",align:null},"Default disable the rate limit")))),(0,r.yg)("h2",{id:"kafka-connect-sink"},"Kafka Connect Sink"),(0,r.yg)("p",null,"If you want to perform streaming ingestion into Hudi format similar to ",(0,r.yg)("inlineCode",{parentName:"p"},"HoodieStreamer"),", but you don't want to depend on Spark,\ntry out the new experimental release of Hudi Kafka Connect Sink. Read the ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect"},"ReadMe"),"\nfor full documentation."))}u.isMDXComponent=!0},45859:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/cdc-2-hudi-d151389758f4ce3fd873c1258b0a8ce5.png"}}]);