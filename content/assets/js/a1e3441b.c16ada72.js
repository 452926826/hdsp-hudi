"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[61416],{15680:(e,a,n)=>{n.d(a,{xA:()=>u,yg:()=>y});var t=n(96540);function o(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function r(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function i(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?r(Object(n),!0).forEach((function(a){o(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function l(e,a){if(null==e)return{};var n,t,o=function(e,a){if(null==e)return{};var n,t,o={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||(o[n]=e[n]);return o}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=t.createContext({}),c=function(e){var a=t.useContext(s),n=a;return e&&(n="function"==typeof e?e(a):i(i({},a),e)),n},u=function(e){var a=c(e.components);return t.createElement(s.Provider,{value:a},e.children)},g="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},p=t.forwardRef((function(e,a){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),g=c(n),p=o,y=g["".concat(s,".").concat(p)]||g[p]||d[p]||r;return n?t.createElement(y,i(i({ref:a},u),{},{components:n})):t.createElement(y,i({ref:a},u))}));function y(e,a){var n=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=p;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l[g]="string"==typeof e?e:o,i[1]=l;for(var c=2;c<r;c++)i[c]=n[c];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}p.displayName="MDXCreateElement"},542:(e,a,n)=>{n.r(a),n.d(a,{contentTitle:()=>i,default:()=>g,frontMatter:()=>r,metadata:()=>l,toc:()=>s});var t=n(58168),o=(n(96540),n(15680));const r={title:"AWS Glue Data Catalog",keywords:["hudi","aws","glue","sync"]},i=void 0,l={unversionedId:"syncing_aws_glue_data_catalog",id:"version-0.15.0/syncing_aws_glue_data_catalog",title:"AWS Glue Data Catalog",description:"Hudi tables can sync to AWS Glue Data Catalog directly via AWS SDK. Piggyback on HiveSyncTool",source:"@site/versioned_docs/version-0.15.0/syncing_aws_glue_data_catalog.md",sourceDirName:".",slug:"/syncing_aws_glue_data_catalog",permalink:"/docs/syncing_aws_glue_data_catalog",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.15.0/syncing_aws_glue_data_catalog.md",tags:[],version:"0.15.0",frontMatter:{title:"AWS Glue Data Catalog",keywords:["hudi","aws","glue","sync"]},sidebar:"docs",previous:{title:"Post-commit Callback",permalink:"/docs/platform_services_post_commit_callback"},next:{title:"DataHub",permalink:"/docs/syncing_datahub"}},s=[{value:"Configurations",id:"configurations",children:[{value:"Avoid creating excessive versions",id:"avoid-creating-excessive-versions",children:[],level:3},{value:"Glue Data Catalog specific configs",id:"glue-data-catalog-specific-configs",children:[],level:3}],level:2},{value:"Other references",id:"other-references",children:[{value:"Running AWS Glue Catalog Sync for Spark DataSource",id:"running-aws-glue-catalog-sync-for-spark-datasource",children:[],level:3},{value:"Running AWS Glue Catalog Sync from EMR",id:"running-aws-glue-catalog-sync-from-emr",children:[],level:3}],level:2}],c={toc:s},u="wrapper";function g(e){let{components:a,...n}=e;return(0,o.yg)(u,(0,t.A)({},c,n,{components:a,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Hudi tables can sync to AWS Glue Data Catalog directly via AWS SDK. Piggyback on ",(0,o.yg)("inlineCode",{parentName:"p"},"HiveSyncTool"),"\n, ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool")," makes use of all the configurations that are taken by ",(0,o.yg)("inlineCode",{parentName:"p"},"HiveSyncTool"),"\nand send them to AWS Glue."),(0,o.yg)("h2",{id:"configurations"},"Configurations"),(0,o.yg)("p",null,"Most of the configurations for ",(0,o.yg)("inlineCode",{parentName:"p"},"AwsGlueCatalogSyncTool")," are shared with ",(0,o.yg)("inlineCode",{parentName:"p"},"HiveSyncTool"),". The example showed in\n",(0,o.yg)("a",{parentName:"p",href:"syncing_metastore"},"Sync to Hive Metastore")," can be used as is for sync with Glue Data Catalog, provided that the hive metastore\nURL (either JDBC or thrift URI) can proxied to Glue Data Catalog, which is usually done within AWS EMR or Glue job environment."),(0,o.yg)("p",null,"For Hudi streamer, users can set"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-shell"},"--sync-tool-classes org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n")),(0,o.yg)("p",null,"For Spark data source writers, users can set"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-shell"},"hoodie.meta.sync.client.tool.class=org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n")),(0,o.yg)("h3",{id:"avoid-creating-excessive-versions"},"Avoid creating excessive versions"),(0,o.yg)("p",null,"Tables stored in Glue Data Catalog are versioned. And by default, every Hudi commit triggers a sync operation if enabled, regardless of having relevant metadata changes.\nThis can lead to too many versions kept in the catalog and eventually failing the sync operation."),(0,o.yg)("p",null,"Meta-sync can be set to conditional - only sync when there are schema change or partition change. This can avoid creating\nexcessive versions in the catalog. Users can enable it by setting "),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"hoodie.datasource.meta_sync.condition.sync=true\n")),(0,o.yg)("h3",{id:"glue-data-catalog-specific-configs"},"Glue Data Catalog specific configs"),(0,o.yg)("p",null,"Sync to Glue Data Catalog can be optimized with other configs like"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"hoodie.datasource.meta.sync.glue.all_partitions_read_parallelism\nhoodie.datasource.meta.sync.glue.changed_partitions_read_parallelism\nhoodie.datasource.meta.sync.glue.partition_change_parallelism\n")),(0,o.yg)("p",null,(0,o.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html"},"Partition indexes")," can also be used by setting"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"hoodie.datasource.meta.sync.glue.partition_index_fields.enable\nhoodie.datasource.meta.sync.glue.partition_index_fields\n")),(0,o.yg)("h2",{id:"other-references"},"Other references"),(0,o.yg)("h3",{id:"running-aws-glue-catalog-sync-for-spark-datasource"},"Running AWS Glue Catalog Sync for Spark DataSource"),(0,o.yg)("p",null,"To write a Hudi table to Amazon S3 and catalog it in AWS Glue Data Catalog, you can use the options mentioned in the\n",(0,o.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-hudi.html#aws-glue-programming-etl-format-hudi-write"},"AWS documentation")),(0,o.yg)("h3",{id:"running-aws-glue-catalog-sync-from-emr"},"Running AWS Glue Catalog Sync from EMR"),(0,o.yg)("p",null,"If you're running HiveSyncTool on an EMR cluster backed by Glue Data Catalog as external metastore, you can simply run the sync from command line like below:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-shell"},"cd /usr/lib/hudi/bin\n\n./run_sync_tool.sh --base-path s3://<bucket_name>/<prefix>/<table_name> --database <database_name> --table <table_name> --partitioned-by <column_name>\n")))}g.isMDXComponent=!0}}]);