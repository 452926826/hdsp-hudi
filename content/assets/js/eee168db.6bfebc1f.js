"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[66327],{15680:(t,e,r)=>{r.d(e,{xA:()=>u,yg:()=>c});var a=r(96540);function n(t,e,r){return e in t?Object.defineProperty(t,e,{value:r,enumerable:!0,configurable:!0,writable:!0}):t[e]=r,t}function o(t,e){var r=Object.keys(t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),r.push.apply(r,a)}return r}function i(t){for(var e=1;e<arguments.length;e++){var r=null!=arguments[e]?arguments[e]:{};e%2?o(Object(r),!0).forEach((function(e){n(t,e,r[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(r,e))}))}return t}function l(t,e){if(null==t)return{};var r,a,n=function(t,e){if(null==t)return{};var r,a,n={},o=Object.keys(t);for(a=0;a<o.length;a++)r=o[a],e.indexOf(r)>=0||(n[r]=t[r]);return n}(t,e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(t);for(a=0;a<o.length;a++)r=o[a],e.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(t,r)&&(n[r]=t[r])}return n}var p=a.createContext({}),s=function(t){var e=a.useContext(p),r=e;return t&&(r="function"==typeof t?t(e):i(i({},e),t)),r},u=function(t){var e=s(t.components);return a.createElement(p.Provider,{value:e},t.children)},d="mdxType",g={inlineCode:"code",wrapper:function(t){var e=t.children;return a.createElement(a.Fragment,{},e)}},m=a.forwardRef((function(t,e){var r=t.components,n=t.mdxType,o=t.originalType,p=t.parentName,u=l(t,["components","mdxType","originalType","parentName"]),d=s(r),m=n,c=d["".concat(p,".").concat(m)]||d[m]||g[m]||o;return r?a.createElement(c,i(i({ref:e},u),{},{components:r})):a.createElement(c,i({ref:e},u))}));function c(t,e){var r=arguments,n=e&&e.mdxType;if("string"==typeof t||n){var o=r.length,i=new Array(o);i[0]=m;var l={};for(var p in e)hasOwnProperty.call(e,p)&&(l[p]=e[p]);l.originalType=t,l[d]="string"==typeof t?t:n,i[1]=l;for(var s=2;s<o;s++)i[s]=r[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},75418:(t,e,r)=>{r.r(e),r.d(e,{contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=r(58168),n=(r(96540),r(15680));const o={title:"Exporter",keywords:["hudi","snapshotexporter","export"],toc:!0},i=void 0,l={unversionedId:"snapshot_exporter",id:"snapshot_exporter",title:"Exporter",description:"Introduction",source:"@site/docs/snapshot_exporter.md",sourceDirName:".",slug:"/snapshot_exporter",permalink:"/docs/next/snapshot_exporter",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/docs/snapshot_exporter.md",tags:[],version:"current",frontMatter:{title:"Exporter",keywords:["hudi","snapshotexporter","export"],toc:!0},sidebar:"docs",previous:{title:"Disaster Recovery",permalink:"/docs/next/disaster_recovery"},next:{title:"Data Quality",permalink:"/docs/next/precommit_validator"}},p=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Arguments",id:"arguments",children:[],level:2},{value:"Examples",id:"examples",children:[{value:"Copy a Hudi dataset",id:"copy-a-hudi-dataset",children:[],level:3},{value:"Export to json or parquet dataset",id:"export-to-json-or-parquet-dataset",children:[],level:3},{value:"Export to json or parquet dataset with transformation/filtering",id:"export-to-json-or-parquet-dataset-with-transformationfiltering",children:[],level:3},{value:"Re-partitioning",id:"re-partitioning",children:[],level:3},{value:"Custom Re-partitioning",id:"custom-re-partitioning",children:[],level:3}],level:2}],s={toc:p},u="wrapper";function d(t){let{components:e,...r}=t;return(0,n.yg)(u,(0,a.A)({},s,r,{components:e,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,"HoodieSnapshotExporter allows you to copy data from one location to another for backups or other purposes.\nYou can write data as Hudi, Json, Orc, or Parquet file formats. In addition to copying data, you can also repartition data\nwith a provided field or implement custom repartitioning by extending a class shown in detail below."),(0,n.yg)("h2",{id:"arguments"},"Arguments"),(0,n.yg)("p",null,"HoodieSnapshotExporter accepts a reference to a source path and a destination path. The utility will issue a\nquery, perform any repartitioning if required and will write the data as Hudi, parquet, or json format."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Argument"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"),(0,n.yg)("th",{parentName:"tr",align:null},"Note"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--source-base-path"),(0,n.yg)("td",{parentName:"tr",align:null},"Base path for the source Hudi dataset to be snapshotted"),(0,n.yg)("td",{parentName:"tr",align:null},"required"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--target-output-path"),(0,n.yg)("td",{parentName:"tr",align:null},"Output path for storing a particular snapshot"),(0,n.yg)("td",{parentName:"tr",align:null},"required"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--output-format"),(0,n.yg)("td",{parentName:"tr",align:null},"Output format for the exported dataset; accept these values: json,parquet,hudi"),(0,n.yg)("td",{parentName:"tr",align:null},"required"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--output-partition-field"),(0,n.yg)("td",{parentName:"tr",align:null},"A field to be used by Spark repartitioning"),(0,n.yg)("td",{parentName:"tr",align:null},"optional"),(0,n.yg)("td",{parentName:"tr",align:null},'Ignored when "Hudi" or when --output-partitioner is specified.The output dataset\'s default partition field will inherent from the source Hudi dataset.')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--output-partitioner"),(0,n.yg)("td",{parentName:"tr",align:null},"A class to facilitate custom repartitioning"),(0,n.yg)("td",{parentName:"tr",align:null},"optional"),(0,n.yg)("td",{parentName:"tr",align:null},'Ignored when using output-format "Hudi"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--transformer-class"),(0,n.yg)("td",{parentName:"tr",align:null},"A subclass of org.apache.hudi.utilities.transform.Transformer. Allows transforming raw source Dataset to a target Dataset (conforming to target schema) before writing."),(0,n.yg)("td",{parentName:"tr",align:null},"optional"),(0,n.yg)("td",{parentName:"tr",align:null},'Ignored when using output-format "Hudi". Available transformers: org.apache.hudi.utilities.transform.SqlQueryBasedTransformer, org.apache.hudi.utilities.transform.SqlFileBasedTransformer, org.apache.hudi.utilities.transform.FlatteningTransformer, org.apache.hudi.utilities.transform.AWSDmsTransformer.')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--transformer-sql"),(0,n.yg)("td",{parentName:"tr",align:null},'sql-query template be used to transform the source before writing. The query should reference the source as a table named "\\<SRC',">",'".'),(0,n.yg)("td",{parentName:"tr",align:null},"optional"),(0,n.yg)("td",{parentName:"tr",align:null},"Is required for SqlQueryBasedTransformer transformer class, ignored in other cases")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"--transformer-sql"),(0,n.yg)("td",{parentName:"tr",align:null},'File with a SQL query to be executed during write. The query should reference the source as a table named "\\<SRC',">",'".'),(0,n.yg)("td",{parentName:"tr",align:null},"optional"),(0,n.yg)("td",{parentName:"tr",align:null},"Is required for SqlFileBasedTransformer, ignored in other cases")))),(0,n.yg)("h2",{id:"examples"},"Examples"),(0,n.yg)("h3",{id:"copy-a-hudi-dataset"},"Copy a Hudi dataset"),(0,n.yg)("p",null,"Exporter scans the source dataset and then makes a copy of it to the target output path."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.15.0.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.15.0.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/hudi/" \\\n  --output-format "hudi"\n')),(0,n.yg)("h3",{id:"export-to-json-or-parquet-dataset"},"Export to json or parquet dataset"),(0,n.yg)("p",null,'The Exporter can also convert the source dataset into other formats. Currently only "json" and "parquet" are supported.'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.15.0.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.15.0.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json"  # or "parquet"\n')),(0,n.yg)("h3",{id:"export-to-json-or-parquet-dataset-with-transformationfiltering"},"Export to json or parquet dataset with transformation/filtering"),(0,n.yg)("p",null,"The Exporter supports custom transformation/filtering on records before writing to json or parquet dataset. This is done by supplying\nimplementation of ",(0,n.yg)("inlineCode",{parentName:"p"},"org.apache.hudi.utilities.transform.Transformer")," via ",(0,n.yg)("inlineCode",{parentName:"p"},"--transformer-class")," option."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.15.0.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.15.0.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --transformer-class "org.apache.hudi.utilities.transform.SqlQueryBasedTransformer" \\\n  --transformer-sql "SELECT substr(rider,1,10) as rider, trip_type as tripType FROM <SRC> WHERE trip_type = \'BLACK\' LIMIT 10" \\\n  --output-format "json"  # or "parquet"\n')),(0,n.yg)("h3",{id:"re-partitioning"},"Re-partitioning"),(0,n.yg)("p",null,"When exporting to a different format, the Exporter takes the ",(0,n.yg)("inlineCode",{parentName:"p"},"--output-partition-field")," parameter to do some custom re-partitioning.\nNote: All ",(0,n.yg)("inlineCode",{parentName:"p"},"_hoodie_*")," metadata fields will be stripped during export, so make sure to use an existing non-metadata field as the output partitions."),(0,n.yg)("p",null,"By default, if no partitioning parameters are given, the output dataset will have no partition."),(0,n.yg)("p",null,"Example:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.15.0.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.15.0.jar \\  \n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partition-field "symbol"  # assume the source dataset contains a field `symbol`\n')),(0,n.yg)("p",null,"The output directory will look like this"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"`_SUCCESS symbol=AMRS symbol=AYX symbol=CDMO symbol=CRC symbol=DRNA ...`\n")),(0,n.yg)("h3",{id:"custom-re-partitioning"},"Custom Re-partitioning"),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"--output-partitioner")," parameter takes in a fully-qualified name of a class that implements ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieSnapshotExporter.Partitioner"),".\nThis parameter takes higher precedence than ",(0,n.yg)("inlineCode",{parentName:"p"},"--output-partition-field"),", which will be ignored if this is provided."),(0,n.yg)("p",null,"An example implementation is shown below:"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"MyPartitioner.java")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'package com.foo.bar;\npublic class MyPartitioner implements HoodieSnapshotExporter.Partitioner {\n\n  private static final String PARTITION_NAME = "date";\n \n  @Override\n  public DataFrameWriter<Row> partition(Dataset<Row> source) {\n    // use the current hoodie partition path as the output partition\n    return source\n        .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n        .repartition(new Column(PARTITION_NAME))\n        .write()\n        .partitionBy(PARTITION_NAME);\n  }\n}\n')),(0,n.yg)("p",null,"After putting this class in ",(0,n.yg)("inlineCode",{parentName:"p"},"my-custom.jar"),", which is then placed on the job classpath, the submit command will look like this:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.15.0.jar,my-custom.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.15.0.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partitioner "com.foo.bar.MyPartitioner"\n')))}d.isMDXComponent=!0}}]);