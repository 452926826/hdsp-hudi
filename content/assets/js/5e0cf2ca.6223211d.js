"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[51766],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>y});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=a.createContext({}),s=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=s(e.components);return a.createElement(c.Provider,{value:t},e.children)},g="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),g=s(n),d=r,y=g["".concat(c,".").concat(d)]||g[d]||p[d]||o;return n?a.createElement(y,l(l({ref:t},u),{},{components:n})):a.createElement(y,l({ref:t},u))}));function y(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,l=new Array(o);l[0]=d;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i[g]="string"==typeof e?e:r,l[1]=i;for(var s=2;s<o;s++)l[s]=n[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},33304:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>l,default:()=>g,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var a=n(87462),r=(n(67294),n(3905));const o={title:"AWS Glue Data Catalog",keywords:["hudi","aws","glue","sync"]},l=void 0,i={unversionedId:"syncing_aws_glue_data_catalog",id:"syncing_aws_glue_data_catalog",title:"AWS Glue Data Catalog",description:"Hudi tables can sync to AWS Glue Data Catalog directly via AWS SDK. Piggyback on HiveSyncTool",source:"@site/docs/syncing_aws_glue_data_catalog.md",sourceDirName:".",slug:"/syncing_aws_glue_data_catalog",permalink:"/docs/next/syncing_aws_glue_data_catalog",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/docs/syncing_aws_glue_data_catalog.md",tags:[],version:"current",frontMatter:{title:"AWS Glue Data Catalog",keywords:["hudi","aws","glue","sync"]},sidebar:"docs",previous:{title:"Data Quality",permalink:"/docs/next/precommit_validator"},next:{title:"DataHub",permalink:"/docs/next/syncing_datahub"}},c=[{value:"Configurations",id:"configurations",children:[{value:"Running AWS Glue Catalog Sync for Spark DataSource",id:"running-aws-glue-catalog-sync-for-spark-datasource",children:[],level:4},{value:"Running AWS Glue Catalog Sync from EMR",id:"running-aws-glue-catalog-sync-from-emr",children:[],level:4}],level:3}],s={toc:c},u="wrapper";function g(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Hudi tables can sync to AWS Glue Data Catalog directly via AWS SDK. Piggyback on ",(0,r.kt)("inlineCode",{parentName:"p"},"HiveSyncTool"),"\n, ",(0,r.kt)("inlineCode",{parentName:"p"},"org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool")," makes use of all the configurations that are taken by ",(0,r.kt)("inlineCode",{parentName:"p"},"HiveSyncTool"),"\nand send them to AWS Glue."),(0,r.kt)("h3",{id:"configurations"},"Configurations"),(0,r.kt)("p",null,"There is no additional configuration for using ",(0,r.kt)("inlineCode",{parentName:"p"},"AwsGlueCatalogSyncTool"),"; you just need to set it as one of the sync tool\nclasses for ",(0,r.kt)("inlineCode",{parentName:"p"},"HoodieStreamer")," and everything configured as shown in ",(0,r.kt)("a",{parentName:"p",href:"syncing_metastore"},"Sync to Hive Metastore")," will\nbe passed along."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"--sync-tool-classes org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n")),(0,r.kt)("h4",{id:"running-aws-glue-catalog-sync-for-spark-datasource"},"Running AWS Glue Catalog Sync for Spark DataSource"),(0,r.kt)("p",null,"To write a Hudi table to Amazon S3 and catalog it in AWS Glue Data Catalog, you can use the options mentioned in the\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-hudi.html#aws-glue-programming-etl-format-hudi-write"},"AWS documentation")),(0,r.kt)("h4",{id:"running-aws-glue-catalog-sync-from-emr"},"Running AWS Glue Catalog Sync from EMR"),(0,r.kt)("p",null,"If you're running HiveSyncTool on an EMR cluster backed by Glue Data Catalog as external metastore, you can simply run the sync from command line like below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cd /usr/lib/hudi/bin\n\n./run_sync_tool.sh --base-path s3://<bucket_name>/<prefix>/<table_name> --database <database_name> --table <table_name> --partitioned-by <column_name>\n")))}g.isMDXComponent=!0}}]);